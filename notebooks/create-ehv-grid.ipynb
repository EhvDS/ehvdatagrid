{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Eindhoven datagrid\n",
    "\n",
    "Notebook to construct the Eindhoven Grid Dataset. \n",
    "Main sections:\n",
    "1. Function definitions\n",
    "1. Action! the actual script\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports, constants, settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import math\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "from shapely.geometry import shape,mapping\n",
    "import json\n",
    "import statistics as stat\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# GRID OUTLINE and RESOLUTION \n",
    "GRID_TOP_LAT = 51.500\n",
    "GRID_LEFT_LON = 5.350\n",
    "GRID_BOT_LAT = 51.400\n",
    "GRID_RIGHT_LON = 5.550\n",
    "TILESIZE_LAT = 0.001\n",
    "TILESIZE_LON = 0.001"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. FUNCTIONS "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Initial empty grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the initial grid, with \"empty\" tiles\n",
    "# Latitude and Longitude are represented as integers, that is * 1000\n",
    "# In: just the settings\n",
    "# Out: empty grid\n",
    "def createEmptyGrid():\n",
    "    lat_list = []\n",
    "    lon_list = []\n",
    "\n",
    "#    for lat in range(int(GRID_BOT_LAT*1000),int(GRID_TOP_LAT*1000),int(TILESIZE_LAT*1000)):\n",
    "#        for lon in range(int(GRID_LEFT_LON*1000),int(GRID_RIGHT_LON*1000),int(TILESIZE_LON*1000)):\n",
    "    for lat in range(int(GRID_BOT_LAT*1000),int(GRID_TOP_LAT*1000),int(TILESIZE_LAT*1000)):\n",
    "        for lon in range(int(GRID_LEFT_LON*1000),int(GRID_RIGHT_LON*1000),int(TILESIZE_LON*1000)):\n",
    "\n",
    "            lat_list.append(lat)\n",
    "            lon_list.append(lon)\n",
    "\n",
    "    df = pd.DataFrame({'lat': lat_list, 'lon': lon_list})\n",
    "    return df\n",
    "\n",
    "# floor a float to 3 decimals\n",
    "def floor3(x):\n",
    "    return (np.floor(x*1000)/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 lng difference means 69.29801236136062 meters\n",
      "0.001 lon difference means 111.31949079369154 meters\n",
      "0.01 lng difference means 694.4995896816404 meters\n",
      "0.01 lon difference means 1113.1949079319584 meters\n"
     ]
    }
   ],
   "source": [
    "def distance_in_meters(lat1, lon1, lat2, lon2):\n",
    "    R = 6378.137; # Radius of earth in KM\n",
    "    dLat = lat2 * math.pi / 180 - lat1 * math.pi / 180\n",
    "    dLon = lon2 * math.pi / 180 - lon1 * math.pi / 180\n",
    "    a = math.sin(dLat/2) * math.sin(dLat/2) + math.cos(lat1 * math.pi / 180) * math.cos(lat2 * math.pi / 180) * math.sin(dLon/2) * math.sin(dLon/2)\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "    d = R * c\n",
    "    return d * 1000; # meters\n",
    "\n",
    "print(\"0.001 lng difference means {} meters\".format(distance_in_meters(51.5,5.300,51.5,5.301)))\n",
    "print(\"0.001 lon difference means {} meters\".format(distance_in_meters(51.400,5.3,51.401,5.3)))\n",
    "print(\"0.01 lng difference means {} meters\".format(distance_in_meters(51.4,5.30,51.4,5.31)))\n",
    "print(\"0.01 lon difference means {} meters\".format(distance_in_meters(51.40,5.3,51.41,5.3)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Neighbourhoods / Buurten info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add buurtcode + buurt to all the tiles intersecting this buurt (buurtshape)\n",
    "# In: buurtcode, buurtshape, df[lat,lon,Buurtcode,BuurtIA]\n",
    "# Out: df[Buurtcode,BuurtIA] extended with the intersection areas of this current buurt with the tile collection\n",
    "def findTilesInBuurt(df, buurtcode, buurtshape):\n",
    "    min_lon, min_lat, max_lon, max_lat = buurtshape.bounds\n",
    "    min_lon = floor3(max(min_lon-TILESIZE_LON,GRID_LEFT_LON))\n",
    "    min_lat = floor3(max(min_lat-TILESIZE_LAT,GRID_BOT_LAT))\n",
    "    max_lon = floor3(min(max_lon+TILESIZE_LON,GRID_RIGHT_LON))\n",
    "    max_lat = floor3(min(max_lat+TILESIZE_LAT,GRID_TOP_LAT))\n",
    "\n",
    "    for lat in np.arange(min_lat, max_lat, TILESIZE_LAT):\n",
    "        for lon in np.arange(min_lon,max_lon, TILESIZE_LON):\n",
    "            tile = Polygon([(lon,lat), (lon,lat+TILESIZE_LAT), (lon+TILESIZE_LON,lat+TILESIZE_LAT), (lon+TILESIZE_LON,lat)])\n",
    "            try:\n",
    "                int_area = buurtshape.intersection(tile).area\n",
    "                if int_area > 0:\n",
    "                    df.loc[(int(1000*lat), int(1000*lon)),'Buurtcode'].append(buurtcode)\n",
    "                    df.loc[(int(1000*lat), int(1000*lon)),'BuurtIA'].append(int_area)\n",
    "            except:\n",
    "                print(\"{0}: ERR tile ({},{}) intersect shape ({})\".format(__name__,lat,lon,buurtshape))\n",
    "    return \n",
    "\n",
    "# assigns each tile to the neighbourhood that intersects it the most\n",
    "# uses findTilesInBuurt\n",
    "# In: df[lat,lon] , dfb = buurten information dataframe\n",
    "# Out: df[Buurtcode, Buurt] \n",
    "def addBuurten(df,dfb):\n",
    "    # make temporary \"Buurt\" and \"BuurtIntersectionArea\" columns\n",
    "    df[\"Buurtcode\"] = 0\n",
    "    df[\"Buurtcode\"] = df[\"Buurtcode\"].apply(lambda x: [])\n",
    "    df[\"BuurtIA\"] = 0 \n",
    "    df[\"BuurtIA\"] = df[\"BuurtIA\"].apply(lambda x: [])\n",
    "    df.set_index(['lat','lon'],inplace=True)\n",
    "\n",
    "    # loop through the list of buurten, mark the intersecting tiles\n",
    "    for i in range(0,len(dfb)):\n",
    "        bpoly = shape(json.loads(dfb.loc[i,'geo_shape']))\n",
    "        ### print(dfb.loc[i,'Buurtcode '])\n",
    "        findTilesInBuurt(df,dfb.loc[i,'Buurtcode '],bpoly)\n",
    "\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    # keep the buurt with max intersection area\n",
    "    df['maxIA'] = [x.index(max(x)) if len(x)>0 else -1 for x in df['BuurtIA']]\n",
    "    df['Buurtcode'] = df.apply(lambda row: row['Buurtcode'][row['maxIA']] if row['maxIA']>=0 else 0, axis=1)\n",
    "    df.drop(['BuurtIA','maxIA'],axis=1,inplace=True)\n",
    "    \n",
    "    # add the name - not efficient, but..\n",
    "    df = df.merge(dfb[['Buurtcode ','Buurtnaam ']],left_on=\"Buurtcode\",right_on=\"Buurtcode \",how=\"left\")\n",
    "    df['Buurtnaam '].fillna('Buiten', inplace=True)\n",
    "    ### df['Buurtnaam '] = df['Buurtnaam '].apply(str.capitalize)\n",
    "    df.rename(columns={'Buurtnaam ': 'Gcb.buurt','Buurtcode': 'Gcb.buurtcode'},inplace=True) \n",
    "    df.drop('Buurtcode ',axis=1,inplace=True)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds number of inhabitants per tile\n",
    "# In: df[cbG.buurtcode,lat] , dfpop = population per buurt \n",
    "# Out: df[nP.pop] \n",
    "def addPopulationDensity(df, dfpop):\n",
    "    dfg = df.groupby('Gcb.buurtcode')['lat'].count().reset_index()\n",
    "    dfg.rename(columns={'lat':'ntiles'},inplace=True)\n",
    "    dfpop.dropna(inplace=True)\n",
    "    dfpop['Buurtcode '] = dfpop['Buurtcode '].fillna(-1).astype(int)\n",
    "    dfpop = dfpop.merge (dfg,left_on='Buurtcode ',right_on='Gcb.buurtcode',how=\"right\")\n",
    "    # dfpop.drop('Buurtcode ',axis=1,inplace=True)\n",
    "    dfpop['Pnb.pop'] = round(dfpop['number of residents'] / dfpop['ntiles'])\n",
    "    dfpop.drop(['number of residents','ntiles'],axis=1,inplace=True)\n",
    "    # compute a density for each tile\n",
    "    df = df.merge(dfpop,on=\"Gcb.buurtcode\",how=\"left\")\n",
    "    df.drop('Buurtcode ',axis=1,inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: keep some of these columns! Electricity Consumption seems to link with AQ :-)\n",
    "Cols2Drop = ['Buurten', 'aantal woningen',\n",
    "       'Totaal aantal huishoudens','gemiddeld elektriciteitsverbruik totaal',\n",
    "       'Doet aan vrijwilligerswerk','Heeft een langdurige ziekte',\n",
    "       'Ervaart een beperkt sociaal netwerk', 'Middelbaar opleidingsniveau %',\n",
    "       'Rapportcijfer dat mensen geven aan veiligheid in hun buurt', 'Totaal 0-14 jaar',\n",
    "       'Totaal 65 jaar en ouder','Totaal 15-64 jaar','Aandeel huishoudens dat gebruik maakt van minimaregeling','Totaal aantal vestigingen',\n",
    "       'Totaal aantal werkzame personen (inclusief uitzendkrachten)',\n",
    "       'Totaal aantal winkelpanden', 'Oppervlakte totaal (in ha)',\n",
    "       'Stedelijkheid']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "RenameDict = {'schaalscore etnische diversiteit':'Pnb.diversity', \n",
    "       'woz-waarde (x 1000)':'Enb.woz', \n",
    "       'Heeft last van vervuilde lucht %':'Pnbp.complains_aq',\n",
    "       'Heeft last van (minstens 1 vorm van) geluidshinder %':'Pnbp.complains_noise',\n",
    "       'verplaatst zich meestal per auto %':'Pnbp.mostly_by_car',\n",
    "       'verplaatst zich meestal te voet %':'Pnbp.mostly_walking', \n",
    "       'Voelt zich niet zo gelukkig of ongelukkig':'Pnbp.unhappy',\n",
    "       'Hoog opleidingsniveau %':'Enbp.high_education',\n",
    "       'Laag opleidingsniveau %':'Enbp.low_education',\n",
    "       'Aandeel personen dat de perceptie heeft dat er veel criminaliteit in de eigen buurt is':'Pnbp.feels_unsafe',\n",
    "       'schaalscore sociale cohesie':'Pnb.cohesion',\n",
    "       'Rapportcijfer dat inwoners geven aan hoe prettig het wonen is in hun buurt':'Pnb.good_life',\n",
    "       'Geregistreerde werkzoekenden UWV zonder dienstverband t.o.v. het aantal 15 t/m 74 jarigen':'Enbp.unemployed',\n",
    "       'Aandeel economisch zelfstandig':'Enbp.independent',\n",
    "       'Gemiddeld besteedbaar huishoudeninkomen (x1000 euro)':'Enb.avg_income',\n",
    "       }\n",
    "\n",
    "def addBuurtStuff(df,dfbi):\n",
    "    df = df.merge(dfbi, left_on=\"Gcb.buurt\", right_on=\"Buurten\", how=\"left\")\n",
    "    df=df.drop(Cols2Drop,axis=1)\n",
    "    df=df.rename(columns=RenameDict)\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Point info over Environment\n",
    "* trees registered in the municipality database\n",
    "* biodiversity observation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trees get aggregated in various ways for each tile: \n",
    "# total number, median height, number of species, most common species\n",
    "\n",
    "def addTreeInfo(df, dfb):\n",
    "   # transform the geo string to lat/lon coordinates - times 1000, to store them as int\n",
    "   dfb[[\"lat\", \"lon\"]] = dfb[\"geo_point_2d\"].str.split(\",\", expand=True).astype(float)\n",
    "   dfb[\"lat\"] = np.floor(dfb[\"lat\"] * 1000).astype(int)\n",
    "   dfb[\"lon\"] = np.floor(dfb[\"lon\"] * 1000).astype(int)\n",
    "\n",
    "   dfb[['Hmin','Hmax']] = dfb['HOOGTE'].str.split('-',expand=True)\n",
    "   dfb['Hmin'] = dfb['Hmin'].str.replace('>','')\n",
    "   dfb['Hmin']=dfb['Hmin'].astype('Int64')\n",
    "   dfb['Hmax']=dfb['Hmax'].astype('Int64')\n",
    "   dfb['HOOGTE'] = (dfb['Hmin'] + dfb['Hmax'])/2\n",
    "\n",
    "   dfb.drop(['geo_point_2d','Hmin','Hmax'],axis=1,inplace=True)\n",
    "   dfb.dropna(inplace=True) # TODO: change this!!\n",
    "\n",
    "   # aggregate by lat and lon, to get one row per tile\n",
    "   grouped = dfb.groupby(['lat', 'lon']).agg({'BOOMSOORT': list, 'BOOMSOORT_NEDERLANDS': list, 'HOOGTE':list, 'PLANTJAAR': list}).reset_index()\n",
    "   grouped['Vn.number_trees'] = [len(l) for l in grouped['BOOMSOORT']]\n",
    "   grouped['Vn.number_tree_species'] = [len(set(l)) for l in grouped['BOOMSOORT_NEDERLANDS']]\n",
    "   grouped['Vc.mode_tree_species'] = [stat.mode(l) for l in grouped['BOOMSOORT_NEDERLANDS']]\n",
    "   ###grouped['Vc.mode_tree_species'] = grouped['Vc.mode_tree_species'].apply(str.capitalize())\n",
    "   # grouped['boom_hoogte_median'] = [stat.median(l) for l in grouped['HOOGTE']] # zou iets anders moeten doen hier, median is beter\n",
    "   grouped['Vn.med_tree_height'] = [stat.median(l) for l in grouped['HOOGTE']]\n",
    "   # grouped['Trees.MedianPlantYear'] = [stat.median(l) for l in grouped['PLANTJAAR']]\n",
    "   grouped.drop(['BOOMSOORT', 'BOOMSOORT_NEDERLANDS', 'HOOGTE','PLANTJAAR'],axis=1,inplace=True)\n",
    "    \n",
    "   # merge met df op lat en lon\n",
    "   df = pd.merge(df, grouped, on = ['lat','lon'], how='left')\n",
    "   return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation data from GBIF\n",
    "# TODO: ask Waarneming.nl to use their data, is much more\n",
    "\n",
    "def addBiodiversityInfo(df,dfobs):\n",
    "    dfobs.rename(columns={'decimalLatitude':'lat', 'decimalLongitude':'lon'},inplace=True)\n",
    "    dfobs=dfobs[dfobs[\"year\"]>=2020].dropna()\n",
    "    dfobs=dfobs[dfobs[\"lat\"]>=GRID_BOT_LAT]\n",
    "    dfobs=dfobs[dfobs[\"lat\"]<=GRID_TOP_LAT]\n",
    "    dfobs=dfobs[dfobs[\"lon\"]>=GRID_LEFT_LON]\n",
    "    dfobs=dfobs[dfobs[\"lon\"]<=GRID_RIGHT_LON]\n",
    "    dfobs[\"lat\"] = np.floor(dfobs[\"lat\"] *1000).astype(int)\n",
    "    dfobs[\"lon\"] = np.floor(dfobs[\"lon\"] *1000).astype(int)\n",
    "    # aggregate by lat and lon, to get one row per tile\n",
    "    grouped = dfobs.groupby(['lat', 'lon']).agg({'species': list}).reset_index()\n",
    "    grouped['Vn.number_species'] = [len(l) for l in grouped['species']]\n",
    "    grouped['Vc.mode_species'] = [stat.mode(l) for l in grouped['species']]\n",
    "    grouped['Vc.mode_species'] = grouped['Vc.mode_species'].apply(str.upper)\n",
    "    grouped.drop(['species'],axis=1,inplace=True)\n",
    "    #merge met df op lat en lon\n",
    "    df = pd.merge(df, grouped, on = ['lat','lon'], how='left')\n",
    "    print(\"{0}: \".format(__name__,df.shape))\n",
    "    df = df.drop_duplicates() # <- why needed?\n",
    "    return df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Air Quality Sensor info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AQ data aggregated in various ways: avg, median, max, min\n",
    "# first per sensor, then per tile\n",
    "# TODO: talk to TNO about the best aggregation\n",
    "\n",
    "def agg_AQ_per_Location(aq):\n",
    "    aq['Date'] = pd.to_datetime(aq['Date'])\n",
    "    aq['Time'] = [pd.Timestamp(t) for t in aq['Time']]\n",
    "    aq['Year'] = aq['Date'].dt.year\n",
    "    aq['Hour'] = [t.hour for t in aq['Time']]\n",
    "    aq['day'] = ['day' if (h>=6 and h<=22) else 'night' for h in aq['Hour']]\n",
    "    aq.drop(['Date','Time','Hour'],axis=1,inplace=True)\n",
    "    #aq.sample(4)\n",
    "\n",
    "    #grouped = aq.groupby(['Location','Year','day']).agg({'PM1': list, 'PM2.5': list, 'PM10':list}).reset_index()\n",
    "    grouped = aq.groupby(['Location','Year']).agg({'PM1': list, 'PM2.5': list, 'PM10':list}).reset_index()\n",
    "    grouped['PM10_med'] = [stat.median(l) for l in grouped['PM10']]\n",
    "    grouped['PM10_avg'] = [stat.mean(l) for l in grouped['PM10']]\n",
    "    grouped['PM10_max'] = [max(l) for l in grouped['PM10']]\n",
    "    grouped['PM10_min'] = [min(l) for l in grouped['PM10']]\n",
    "    grouped['PM10_range'] = grouped['PM10_max'] - grouped['PM10_min']\n",
    "\n",
    "    grouped.drop(['PM1', 'PM2.5', 'PM10'],axis=1,inplace=True)\n",
    "\n",
    "    aqlocs = pd.DataFrame(columns=['LocationID'])\n",
    "    aqlocs['LocationID']=grouped['Location'].unique()\n",
    "    aqlocs.set_index('LocationID',inplace=True)\n",
    "\n",
    "    #\n",
    "    grouped.set_index(['Location','Year'],inplace=True)\n",
    "    for i in grouped.index:\n",
    "        colname = \"Vn.PM10_med_\" + str(i[1]) \n",
    "        aqlocs.loc[i[0],colname] = grouped.loc[i,'PM10_med']\n",
    "        colname = \"Vn.PM10_avg_\" + str(i[1]) \n",
    "        aqlocs.loc[i[0],colname] = grouped.loc[i,'PM10_avg']\n",
    "        colname = \"Vn.PM10_max_\" + str(i[1]) \n",
    "        aqlocs.loc[i[0],colname] = grouped.loc[i,'PM10_max']\n",
    "    #\n",
    "    aqlocs.reset_index(inplace=True)\n",
    "    return aqlocs\n",
    "\n",
    "def agg_AQ_per_Tile(df,aqlocs,locs):\n",
    "    locs=locs[['ID','Lat','Lon']]\n",
    "    g=aqlocs.merge(locs,how='left',left_on='LocationID',right_on='ID')\n",
    "    g[\"lat\"] = np.floor(g[\"Lat\"] * 1000).astype(int,errors='ignore')\n",
    "    g[\"lon\"] = np.floor(g[\"Lon\"] * 1000).astype(int,errors='ignore')\n",
    "\n",
    "    aqtiles = g.groupby(['lat','lon']).agg(list).reset_index()\n",
    "    COLS_2_AGG_BY_MEDIAN = ['Vn.PM10_med_2020','Vn.PM10_med_2021','Vn.PM10_med_2022']\n",
    "    COLS_2_AGG_BY_MEAN = ['Vn.PM10_avg_2020','Vn.PM10_avg_2021','Vn.PM10_avg_2022']\n",
    "    COLS_2_AGG_BY_MAX = ['Vn.PM10_max_2020','Vn.PM10_max_2021','Vn.PM10_max_2022']\n",
    "    ##COLS_2_DROP = list(set(aqtiles.columns) - {'lat','lon'} - set(COLS_2_AGG_BY_MEDIAN) - set(COLS_2_AGG_BY_MAX))\n",
    "    COLS_2_DROP = list(set(aqtiles.columns) - {'lat','lon'} - set(COLS_2_AGG_BY_MEAN))\n",
    "    for c in COLS_2_AGG_BY_MEDIAN:\n",
    "        aqtiles[c] = [stat.median(l) for l in aqtiles[c]]\n",
    "    for c in COLS_2_AGG_BY_MEAN:\n",
    "        aqtiles[c] = [stat.mean(l) for l in aqtiles[c]]\n",
    "    for c in COLS_2_AGG_BY_MAX:\n",
    "        aqtiles[c] = [max(l) for l in aqtiles[c]]\n",
    "    aqtiles.drop(COLS_2_DROP,axis=1,inplace=True)\n",
    "    aqtiles['lat']=aqtiles['lat'].astype('Int64')\n",
    "    aqtiles['lon']=aqtiles['lon'].astype('Int64')\n",
    "\n",
    "    #merge met df op lat en lon\n",
    "    df = pd.merge(df, aqtiles, on = ['lat','lon'], how='left')\n",
    "    return df "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Green and Grey information from ODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_intersections_with_tiles(df, polygon, col):\n",
    " ###   print(\"Polygon \", polygon.bounds)\n",
    "    min_lat, min_lon, max_lat, max_lon = polygon.bounds\n",
    "    min_lon = floor3(max(min_lon-TILESIZE_LON,GRID_LEFT_LON))\n",
    "    min_lat = floor3(max(min_lat-TILESIZE_LAT,GRID_BOT_LAT))\n",
    "    max_lon = floor3(min(max_lon+TILESIZE_LON,GRID_RIGHT_LON-0.001))\n",
    "    max_lat = floor3(min(max_lat+TILESIZE_LAT,GRID_TOP_LAT-0.001))\n",
    "  ###  print(min_lat,max_lat,min_lon,max_lon)\n",
    "    ntiles=0\n",
    "    ta=0\n",
    "    for lat in np.arange(min_lat, max_lat, TILESIZE_LAT):\n",
    "        for lon in np.arange(min_lon,max_lon, TILESIZE_LON):\n",
    "            \n",
    "            latlng_coords = [(lat,lon), (lat+TILESIZE_LAT,lon), (lat+TILESIZE_LAT,lon+TILESIZE_LON), (lat,lon+TILESIZE_LON)]\n",
    "            tile = Polygon(latlng_coords)\n",
    "           ### print(\"Tile: \",tile)\n",
    "            intersection = polygon.intersection(tile)\n",
    "            int_area = min(intersection.area,polygon.area,tile.area) # <-- to prevent tiny area estimation errors\n",
    "\n",
    "            if int_area > 0:\n",
    "                try:\n",
    "                    ntiles=ntiles+1\n",
    "                    a = df.loc[(int(1000*lat), int(1000*lon)),col]\n",
    "                    df.loc[(int(1000*lat), int(1000*lon)),col] =  a + int_area\n",
    "                    ta=ta+int_area\n",
    "                except:\n",
    "                    print(\"ERROR IN INTERSECTION of Polygon: \",polygon,\"\\nand Tile \",tile,\"\\ndf-index is \", int(1000*lat), int(1000*lon))\n",
    "                    pass   ### TODO: fix this a different way! sometimes the lon or lat get out of bounds\n",
    "    return ntiles,ta\n",
    "\n",
    "#########################################\n",
    "def ReverseLatLng(p):\n",
    "    ###print(\"reversing \",p)\n",
    "    coords = p.exterior.coords[:]\n",
    "    new_coords = [(y, x) for x, y in coords]\n",
    "    return Polygon(new_coords)\n",
    "\n",
    "def GeoShapes2Polygons(lgs):\n",
    "    lp = []\n",
    "    for i in range(0,len(lgs)):\n",
    "        try:\n",
    "            js = json.loads(lgs[i])\n",
    "            if (js['type'] == 'Polygon'):\n",
    "                lp.append(ReverseLatLng(Polygon(js['coordinates'][0])))\n",
    "            elif (js['type'] == 'MultiPolygon'):\n",
    "                mp=shape(js)\n",
    "                ###for p in mp.geoms:\n",
    "                ###    print(mapping(p),type(mapping(p)))\n",
    "                for geometry in mp.geoms:\n",
    "                    lp.append(ReverseLatLng(Polygon(geometry)))\n",
    "            else:\n",
    "                print(\"it is a \",js['type'])\n",
    "        except:\n",
    "            print(\"Shape \",i, \": ERR \",lgs[i])\n",
    "            pass\n",
    "    return lp\n",
    "\n",
    "\n",
    "def addShapeIntersection(df,col,ssh):\n",
    "    # make \n",
    "    df[col] = 0 \n",
    "    df.set_index(['lat','lon'],inplace=True)\n",
    "\n",
    "\n",
    "    # only polygons..abs\n",
    "    print(len(ssh), \" geoshapes\")\n",
    "    ssh = GeoShapes2Polygons(ssh)\n",
    "    print(len(ssh), \" polygons\")\n",
    "\n",
    "    # loop through the list of shapes, mark the intersecting tiles\n",
    "    for i in range(0,len(ssh)):\n",
    "        try:\n",
    "            ###print(\"geo_shape \",i,\" : \", ssh[i])\n",
    "            #json_obj = json.loads(ssh[i])\n",
    "            ###print(\"Json \",i,\" : \", json_obj)\n",
    "            #bpoly = Polygon(json_obj['coordinates'][0])\n",
    "            ###print(\"Shape \",i,\" : \", bpoly)\n",
    "            bpoly=ssh[i]\n",
    "            ntiles, totalarea = compute_intersections_with_tiles(df,bpoly,col)\n",
    "            if (floor3(totalarea )> floor3(bpoly.area)):\n",
    "            ####if (ntiles>0):\n",
    "                print(\"Shape \",i,bpoly,\" \\n--------- intersects \", ntiles, \"tiles. Intarea: \",totalarea,\" out of \",bpoly.area)\n",
    "        except:\n",
    "            print(\"Shape \",i, \": ERR \",ssh[i])\n",
    "            pass\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    # transform the area sum to percentage\n",
    "    tilearea = TILESIZE_LAT * TILESIZE_LON\n",
    "    ## print(\"Tilearea in degrees: \",tilearea)\n",
    "    df[col] = df[col] / tilearea\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 Bioclim variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temperature and humidity information from WORLDCLIM\n",
    "# TODO: extract the 30s resolution data, now it is 5 m because of timeout (hardware limitation?)\n",
    "\n",
    "def getBioclimVars(aggform,coltemp,colprec):\n",
    "    \n",
    "    import os\n",
    "    import subprocess\n",
    "    import sys\n",
    "    os.environ['LATLONRES'] = \"5m\" # visible in this process + all children\n",
    "    from latlon_utils import get_climate\n",
    "    \n",
    "    lat_list = []\n",
    "    lon_list = []\n",
    "    coltemp_list=[]\n",
    "    colprec_list=[]\n",
    "\n",
    "    for lat10x in np.arange(GRID_BOT_LAT,GRID_TOP_LAT,TILESIZE_LAT*10):\n",
    "        for lon10x in np.arange(GRID_LEFT_LON,GRID_RIGHT_LON,TILESIZE_LON*10):\n",
    "            tp = get_climate(lat10x + TILESIZE_LAT*5, lon10x + TILESIZE_LON*5)\n",
    "            tpt = tp[('tavg',aggform)]\n",
    "            tpp = tp[('prec',aggform)]\n",
    "            for lat in np.arange(lat10x,lat10x+TILESIZE_LAT*10,TILESIZE_LAT):\n",
    "                for lon in np.arange(lon10x,lon10x+TILESIZE_LON*10,TILESIZE_LON):\n",
    "                    lat_list.append(lat)\n",
    "                    lon_list.append(lon)\n",
    "                    coltemp_list.append(tpt)\n",
    "                    colprec_list.append(tpp)\n",
    "        print(lat10x)\n",
    "\n",
    "    dftp = pd.DataFrame({'lat': lat_list, 'lon': lon_list, coltemp:coltemp_list,colprec:colprec_list})\n",
    "    dftp['lat'] = [int(np.floor(x * 1000)) for x in dftp['lat']]\n",
    "    dftp['lon'] = [int(np.floor(x * 1000)) for x in dftp['lon']]\n",
    "    dftp.drop_duplicates(subset=['lat', 'lon'], inplace=True)\n",
    "    return dftp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addWorldClim(df,aggform,coltemp,colprec):\n",
    "    dftp = getBioclimVars(aggform,coltemp,colprec)\n",
    "    #merge met df op lat en lon\n",
    "    df = pd.merge(df, dftp, on = ['lat','lon'], how='left')\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7 Generate the codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExplanationDict = {'lat':'latitude of the bottom-left tile corner x 1000', \n",
    "                     'lon':'longitude of the bottom-left tile corner x 1000', \n",
    "                   'Enb.avg_income':'the average income per working individual, in this neighbourhood', \n",
    "                   'Enb.woz':'average house value', \n",
    "                   'Enbp.high_education':'percentage of people with high education',\n",
    "                   'Enbp.independent':'percentage of people working', \n",
    "                   'Enbp.low_education':'percentage of people with low education', \n",
    "                   'Enbp.unemployed':'unemployment percentage',\n",
    "       'Gcb.buurt':'the name of the neighbourhood where this tile belongs to', \n",
    "       'Gcb.buurtcode':'the unique identifier code of the neighbourhood', \n",
    "       'Pnb.cohesion':'perceived social cohesion score', \n",
    "       'Pnb.diversity':'ethnical diversity score',\n",
    "       'Pnb.good_life':'score for perceived good life', \n",
    "       'Pnb.pop':'number of residents/tile in this neigbourhood', \n",
    "       'Pnbp.complains_aq':'percentage of people compaining about air quality', \n",
    "       'Pnbp.complains_noise':'percentage of people compaining about noise',\n",
    "       'Pnp.complains_noise':'number of omplaints about noise',\n",
    "       'Pnbp.feels_unsafe':'percentage of people that feel unsafe', \n",
    "       'Pnbp.mostly_by_car':'percentage of people that move mostly by car', \n",
    "       'Pnbp.mostly_walking':'percentage of people that move mostly by walking',\n",
    "       'Pnbp.unhappy':'percentage of people that feel unhappy', \n",
    "       'Vc.milieuzone':'can trucks drive through (True/False)', \n",
    "       'Vc.mode_tree_species':'the most common tree species in this tile',\n",
    "       'Vn.number_wild_species':'number of different observed plant or animal species in this tile', \n",
    "       'Vc.mode_species':'the most observed plant or animal species',\n",
    "       'Vn.PM10_avg_2020':'the average PM10 value of all the measurements in 2020', \n",
    "       'Vn.PM10_avg_2021':'',\n",
    "       'Vn.PM10_avg_2022':'', \n",
    "       'Vn.PM10_med_2020':'the median PM10 value of all the measurements in 2020, on the sensors of this tile', \n",
    "       'Vn.PM10_med_2021':'the median PM10 value of all the measurements in 2021, on the sensors of this tile',\n",
    "       'Vn.PM10_med_2022':'the median PM10 value of all the measurements in 2022, on the sensors of this tile', \n",
    "       'Vn.med_tree_height':'the median height of the trees on this tile', \n",
    "       'Vn.number_species':'the number of plant, animals, birds and insects species observed on this tile',\n",
    "       'Vn.number_tree_species':'the number of different tree species', \n",
    "       'Vn.number_trees':'the total number of trees on this tile', \n",
    "       'Vn.prec_ann':'the average rainfall at the centerpoint of this tile in 2022',\n",
    "       'Vn.tavg_ann':'the average temperature at the centerpoint of this tile in 2022', \n",
    "       'Vnp.Green':'the percentage of public green space', \n",
    "       'Inp.Grey':'the percentage of streets and pavements',\n",
    "       'Vnp.Grey':''\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EIC = \"EIC\"\n",
    "ODE = \"ODE\"\n",
    "GBIF = \"GBIF\"\n",
    "TNO = \"TNO\"\n",
    "BIOCLIM = \"WORLDCLIM\"\n",
    "\n",
    "SourceDict = {'lat':'', 'lon':'', \n",
    "      'Enb.avg_income':EIC, \n",
    "      'Enb.woz':EIC,\n",
    "      'Enbp.high_education':EIC,\n",
    "      'Enbp.independent':EIC,\n",
    "      'Enbp.low_education':EIC,\n",
    "      'Enbp.unemployed':EIC,\n",
    "      'Gcb.buurt':ODE, \n",
    "      'Gcb.buurtcode':ODE,\n",
    "      'Pnb.cohesion':EIC,\n",
    "      'Pnb.diversity':EIC,\n",
    "      'Pnb.good_life':EIC,\n",
    "      'Pnb.pop':EIC,\n",
    "      'Pnbp.complains_aq':EIC, \n",
    "      'Pnbp.complains_noise':EIC,\n",
    "      'Pnbp.feels_unsafe':EIC,\n",
    "      'Pnbp.mostly_by_car':EIC,\n",
    "       'Pnbp.mostly_walking':EIC,\n",
    "       'Pnbp.unhappy':EIC,\n",
    "       'Vc.milieuzone':ODE,\n",
    "       'Vc.mode_tree_species':ODE,\n",
    "       'Vn.number_wild_species':GBIF, \n",
    "       'Vc.mode_species':GBIF,\n",
    "       'Vn.PM10_avg_2020':TNO, \n",
    "       'Vn.PM10_avg_2021':TNO,\n",
    "       'Vn.PM10_avg_2022':TNO, \n",
    "       'Vn.PM10_med_2020':TNO, \n",
    "       'Vn.PM10_med_2021':TNO,\n",
    "       'Vn.PM10_med_2022':TNO,\n",
    "       'Vn.med_tree_height':ODE + \"- bomen\", \n",
    "       'Vn.number_species': GBIF,\n",
    "       'Vn.number_tree_species': ODE + \"- bomen\",  \n",
    "       'Vn.number_trees': ODE+ \"- bomen\",  \n",
    "       'Vn.prec_ann':BIOCLIM,\n",
    "       'Vn.tavg_ann':BIOCLIM, \n",
    "       'Vnp.Green':ODE + \"- openbaar groen\", \n",
    "       'Inp.Grey':ODE + \"- bestrating\",\n",
    "       'Vnp.Grey':''\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "UnitDict = {'lat':'latitude degree ', \n",
    "            'lon':'longitude degree', \n",
    "      'Enb.avg_income':'euro', \n",
    "      'Enb.woz':'thousand euro',\n",
    "      'Enbp.high_education':'%',\n",
    "      'Enbp.independent':'%',\n",
    "      'Enbp.low_education':'%',\n",
    "      'Enbp.unemployed':'%',\n",
    "      'Gcb.buurt':'', \n",
    "      'Gcb.buurtcode':'',\n",
    "      'Pnb.cohesion':'score from 1 to 10',\n",
    "      'Pnb.diversity':'computed index between 0 and 1',\n",
    "      'Pnb.good_life':'score from 1 to 10',\n",
    "      'Pnb.pop':'residents',\n",
    "      'Pnbp.complains_aq':'%', \n",
    "      'Pnbp.complains_noise':'%',\n",
    "      'Pnbp.feels_unsafe':'%',\n",
    "      'Pnbp.mostly_by_car':'%',\n",
    "       'Pnbp.mostly_walking':'%',\n",
    "       'Pnbp.unhappy':'%',\n",
    "       'Vc.milieuzone':'True/False',\n",
    "       'Vc.mode_tree_species':'',\n",
    "       'Vn.number_wild_species':'species', \n",
    "       'Vc.mode_species':'',\n",
    "       'Vn.PM10_avg_2020':'$\\mu/m^3$', \n",
    "       'Vn.PM10_avg_2021':'$\\mu/m^3$',\n",
    "       'Vn.PM10_avg_2022':'$\\mu/m^3$', \n",
    "       'Vn.PM10_med_2020':'$\\mu/m^3$', \n",
    "       'Vn.PM10_med_2021':'$\\mu/m^3$',\n",
    "       'Vn.PM10_med_2022':'$\\mu/m^3$', \n",
    "       'Vn.med_tree_height':'m', \n",
    "       'Vn.number_species': 'species',\n",
    "       'Vn.number_tree_species': 'species',  \n",
    "       'Vn.number_trees': 'trees',  \n",
    "       'Vn.prec_ann':'mm',\n",
    "       'Vn.tavg_ann':'$\\degree C$', \n",
    "       'Vnp.Green':'%', \n",
    "       'Inp.Grey':'%',\n",
    "       'Vnp.Grey':''\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefixContains(varname,caracter):\n",
    "    return (caracter in varname.split(\".\")[0])\n",
    "\n",
    "# TODO: separate the scale functions from the print\n",
    "def scaleTo01(l):\n",
    "    vmax = np.max(l) #TODO: fix!\n",
    "    if (vmax>10): # very bad heuristics to determine if a variable is in 0-100 or 0-1 scale\n",
    "        l = l / 100\n",
    "        vmax = np.max(l)\n",
    "    if (vmax < 1.0):\n",
    "        return l\n",
    "    #print(\"SCALE.. vmax = \", vmax)\n",
    "    l01 = l.copy()\n",
    "    for i in range(len(l)):\n",
    "        if (not np.isnan(l[i])):\n",
    "            if l[i] >=0:\n",
    "                l01[i] = np.floor(l[i]*100/vmax)/100 \n",
    "                #print(\"! \", l[i],l01[i])\n",
    "            else: \n",
    "                #print(\"? \", l[i])\n",
    "                l01[i] = np.nan\n",
    "    return l01\n",
    "\n",
    "def scaleTo100(l,maxp):\n",
    "    vmax = np.max(l) #TODO: fix!\n",
    "    if (vmax < 10): # very bad heuristics to determine if a variable is in 0-100 or 0-1 scale\n",
    "        l = [x*100 for x in l]\n",
    "    if (vmax <= maxp):\n",
    "        return l\n",
    "    l100 = l.copy()\n",
    "    for i in range(len(l)):\n",
    "        if (not np.isnan(l[i])):\n",
    "            if (l[i] >=0) and (l[i] <=maxp):\n",
    "                l100[i] = l[i]\n",
    "            else: \n",
    "                l100[i] = np.nan\n",
    "    return l100\n",
    "\n",
    "def string2Float_older(l):\n",
    "    GARBAGE_CHARS = {'.','-'} \n",
    "    lf = [np.nan if ((v=='.') or (v=='-')) else v for v in l]\n",
    "    lf = [v.replace(',','.') if isinstance(v,str) else v for v in lf]\n",
    "    lf = [(np.nan if (v == np.nan) else np.floor(float(v)*100)/100)  for v in lf]\n",
    "    return lf\n",
    "\n",
    "def string2Float(l):\n",
    "    lf = l.copy()\n",
    "    for i in range(len(l)):\n",
    "        if (not (l[i] is None)):\n",
    "            try:\n",
    "                lf[i] = l[i].replace(',','.')\n",
    "                lf[i] = np.floor(float(lf[i])*100)/100\n",
    "                ##print(\"! \", l[i],l01[i])\n",
    "            except: \n",
    "                ###print(\"? \", l[i])\n",
    "                lf[i] = np.nan\n",
    "    return lf\n",
    "\n",
    "def getRange(l,lname):\n",
    "    try:\n",
    "        if prefixContains(lname,'c'):\n",
    "            vals = set(l) - {np.nan}\n",
    "            mi = min(vals)\n",
    "            ma = max(vals)\n",
    "        else:\n",
    "            mi = np.floor(np.min(l)*100)/100\n",
    "            ma = np.floor(np.max(l)*100)/100\n",
    "    except:\n",
    "        ###print(\"{} - ERR\".format(__name__))\n",
    "        mi=\"\"\n",
    "        ma=\"\"\n",
    "    return mi,ma\n",
    "\n",
    "\n",
    "def printGridInMarkdown(df):\n",
    "    print(\"| Variable | Explanation | Unit | Range | Source |\")\n",
    "    print(\"| :--- | :--- | :--- | :--- | :--- | \")\n",
    "\n",
    "    for x in df.columns:\n",
    "        ###print(x,df[x].dtype)\n",
    "        if (prefixContains(x,'n')) and (df[x].dtype=='object'):\n",
    "            df[x] = string2Float(df[x])\n",
    "            ###print(\"s2f --\",x,df[x].count())\n",
    "        if (prefixContains(x,'p')): # percent\n",
    "            ###df[x] = scaleTo01(df[x])\n",
    "            df[x] = scaleTo100(df[x],90)\n",
    "            ###print(\"p -- \",x,df[x].count())\n",
    "        \n",
    "        mi,ma = getRange(df[x],x)\n",
    "        if prefixContains(x,'b'):\n",
    "            print(\"| *{0}* | {3} | {4}|{1} - {2}| {5}| \".format(x,mi,ma,ExplanationDict[x],UnitDict[x],SourceDict[x]))\n",
    "        else:\n",
    "            print(\"| **{0}** | {3} | {4}|{1} - {2}| {5}|\".format(x,mi,ma,ExplanationDict[x],UnitDict[x], SourceDict[x]))\n",
    "    return\n",
    "\n",
    "def printGridInLatex(df):\n",
    "    print(\"Variable & Explanation & Unit & Range & Source \\\\\")\n",
    "    print(\"\\hline \")\n",
    "\n",
    "    for x in df.columns:\n",
    "        ###print(x,df[x].dtype)\n",
    "        if (prefixContains(x,'n')) and (df[x].dtype=='object'):\n",
    "            df[x] = string2Float(df[x])\n",
    "            ###print(\"s2f --\",x,df[x].count())\n",
    "        if (prefixContains(x,'p')): # percent\n",
    "            ###df[x] = scaleTo01(df[x])\n",
    "            df[x] = scaleTo100(df[x],90)\n",
    "            ###print(\"p -- \",x,df[x].count())\n",
    "        \n",
    "        mi,ma = getRange(df[x],x)\n",
    "        if prefixContains(x,'b'):\n",
    "            print(\" \\{\\\\em {0} \\} & {3} & {4}|{1} - {2}& {5}\\\\\\\\ \".format(x,mi,ma,ExplanationDict[x],UnitDict[x],SourceDict[x]))\n",
    "        else:\n",
    "            print(\" \\{\\\\bf {0} \\} & {3} & {4}|{1} - {2}& {5}\\\\\\\\\".format(x,mi,ma,ExplanationDict[x],UnitDict[x], SourceDict[x]))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d = df[[\"Enb.avg_income\",\"Enb.woz\"]]\n",
    "#d.sample(5)\n",
    "#df = dfsaved.copy()\n",
    "#he = scaleTo01(string2Float(df[\"Enbp.high_education\"]))\n",
    "#np.max(he)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ACTION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Create the grid by reading in the data sources and processing them one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created the empty grid \n",
      " (20000, 2) {'lat', 'lon'}\n"
     ]
    }
   ],
   "source": [
    "# SETTINGS\n",
    "# datasets from Open Data Eindhoven\n",
    "ode_folder = \"data/ode-downloads-mar-2023/\"\n",
    "\n",
    "# TODO: add \"ode-downloads-mar-2023/meldingen-openbare-ruimte.csv\"\n",
    "\n",
    "ALLVARS=[]\n",
    "\n",
    "# empty grid\n",
    "df = createEmptyGrid()\n",
    "print(\"Created the empty grid \\n\", df.shape, set(df.columns)-set(ALLVARS))\n",
    "ALLVARS = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added buurten \n",
      " (20000, 4) {'Gcb.buurt', 'Gcb.buurtcode'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# assign each tile to a neighbourhoud\n",
    "dfb = pd.read_csv(ode_folder + \"buurten.csv\",sep=';') [['Buurtcode ', 'Buurtnaam ', 'geo_shape']]\n",
    "df = addBuurten(df,dfb)\n",
    "print(\"Added buurten \\n\", df.shape, set(df.columns)-set(ALLVARS))\n",
    "ALLVARS = df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added population info \n",
      " (20000, 5) {'Pnb.pop'}\n",
      "Added Buurtgegevens \n",
      " (20000, 20) {'Enb.woz', 'Pnbp.mostly_by_car', 'Enbp.high_education', 'Pnbp.unhappy', 'Enbp.low_education', 'Pnbp.complains_noise', 'Pnbp.mostly_walking', 'Pnbp.feels_unsafe', 'Pnb.cohesion', 'Enb.avg_income', 'Pnb.good_life', 'Enbp.independent', 'Pnb.diversity', 'Pnbp.complains_aq', 'Enbp.unemployed'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# add demographic information\n",
    "dfpop = pd.read_csv(ode_folder + \"buurt_bevolking.csv\",sep=\";\") [['Buurtcode ','number of residents']]\n",
    "df = addPopulationDensity(df,dfpop)\n",
    "print(\"Added population info \\n\", df.shape, set(df.columns)-set(ALLVARS))\n",
    "ALLVARS = df.columns\n",
    "\n",
    "# add more info\n",
    "dfbi = pd.read_csv(ode_folder + \"Buurtgegevens - Buurten.csv\",sep=\";\")\n",
    "df = addBuurtStuff(df,dfbi)\n",
    "print(\"Added Buurtgegevens \\n\", df.shape, set(df.columns)-set(ALLVARS))\n",
    "ALLVARS = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added Trees \n",
      " (20000, 24) {'Vn.med_tree_height', 'Vc.mode_tree_species', 'Vn.number_trees', 'Vn.number_tree_species'}\n",
      "__main__: \n",
      "Added gbif information \n",
      " (20000, 26) {'Vn.number_species', 'Vc.mode_species'}\n"
     ]
    }
   ],
   "source": [
    "# tree info\n",
    "dfb = pd.read_csv(ode_folder + \"bomen.csv\",quotechar='\"',sep=';') \n",
    "                              # 'geo_point_2d', 'HOOGTE'\n",
    "COLUMNS_TO_DROP = ['OBJECTID', 'BOOMNUMMER', 'GEOVISIA_ID', 'EIGENAAR', 'BEHEERDER',\n",
    "      'STATUS_TER_INDICATIE', 'BOOM_DEF_AFWEZIG', 'EINDBEELD', 'BOOMSOORT_VARIETEIT',\n",
    "      'PLANTWIJZE', 'BOOMROOSTER', 'VERLICHTING', 'EPR_AANWEZIG',\n",
    "      'EPR_PREVENTIEF_INSPECTEREN', 'EPR_BESTRIJDINGSMETHODE', 'EPR_STADIUM',\n",
    "      'EPR_DATUM_CONSTATERING', 'EPR_RISICOPROFIEL',\n",
    "      'EPR_DATUM_MECH_BESTREDEN', 'EPR_DATUM_1_BIO_BESTRIJDING',\n",
    "      'EPR_DATUM_2_BIO_BESTRIJDING', 'NAZORGBOOM', 'PROJECTNAAM', 'geo_shape']\n",
    "dfb.drop(COLUMNS_TO_DROP,axis=1,inplace=True)\n",
    "df = addTreeInfo(df, dfb)\n",
    "print(\"Added Trees \\n\", df.shape, set(df.columns)-set(ALLVARS))\n",
    "ALLVARS = df.columns\n",
    "\n",
    "# gbif info\n",
    "file_gbif = \"./data/gbif/0151022-230224095556074.csv\"\n",
    "dfobs = pd.read_csv(file_gbif,sep=\"\\t\")[['species','decimalLatitude', 'decimalLongitude', 'year']]\n",
    "df = addBiodiversityInfo(df,dfobs)\n",
    "print(\"Added gbif information \\n\", df.shape, set(df.columns)-set(ALLVARS))\n",
    "ALLVARS = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45588  geoshapes\n",
      "45604  polygons\n",
      "Added public green info  (20000, 27) \n",
      " {'Vnp.Green'}\n"
     ]
    }
   ],
   "source": [
    "# openbaar groen\n",
    "dfg = pd.read_csv(ode_folder + \"openbaar-groen0.csv\",sep=\";\")[['geo_shape']]\n",
    "dfg = dfg.dropna().reset_index()\n",
    "df = addShapeIntersection(df,\"Vnp.Green\",dfg[\"geo_shape\"])\n",
    "print(\"Added public green info \", df.shape, \"\\n\", set(df.columns)-set(ALLVARS))\n",
    "ALLVARS = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102792  geoshapes\n",
      "102793  polygons\n",
      "Added street/stone info  (20000, 28) \n",
      " {'Inp.Grey'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# openbaar grijs\n",
    "dfg = pd.read_csv(ode_folder + \"bestrating.csv\",sep=\";\")[['geo_shape']]\n",
    "dfg = dfg.dropna().reset_index()\n",
    "df = addShapeIntersection(df,\"Inp.Grey\",dfg[\"geo_shape\"])\n",
    "print(\"Added street/stone info \", df.shape, \"\\n\", set(df.columns)-set(ALLVARS))\n",
    "ALLVARS = df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2  geoshapes\n",
      "2  polygons\n",
      "Added milieuzone 1/0  (20000, 29) \n",
      " {'Vc.milieuzone'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# milieuzones\n",
    "sfg = pd.read_csv(ode_folder + \"milieuzone.csv\",sep=\";\")['Geo Shape']\n",
    "df = addShapeIntersection(df,\"Vc.milieuzone\",sfg)\n",
    "df[\"Vc.milieuzone\"] = [1 if x > 0.5 else 0 for x in df[\"Vc.milieuzone\"]]\n",
    "print(\"Added milieuzone 1/0 \", df.shape, \"\\n\", set(df.columns)-set(ALLVARS))\n",
    "ALLVARS = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added aggregated AQ sensor data  (20000, 32) \n",
      " {'Vn.PM10_avg_2022', 'Vn.PM10_avg_2020', 'Vn.PM10_avg_2021'}\n"
     ]
    }
   ],
   "source": [
    "# air quality \n",
    "aq = pd.read_csv(r\"./data/AQ-2019-tm-feb2023-by-Skyler/full_measurement.csv\",quotechar='\"',sep=',')\n",
    "aqlocs = agg_AQ_per_Location(aq) # <-- duurt 5+ min \n",
    "locs=pd.read_csv(r\"./data/AQ-2019-tm-feb2023-by-Skyler/locaties-airboxen.csv\",quotechar='\"',sep=';')\n",
    "df = agg_AQ_per_Tile(df,aqlocs,locs)\n",
    "print(\"Added aggregated AQ sensor data \", df.shape, \"\\n\", set(df.columns)-set(ALLVARS))\n",
    "ALLVARS = df.columns\n",
    "df.to_csv(\"ehv-grid-mar23-without-bioclim.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.4\n",
      "51.41\n",
      "51.419999999999995\n",
      "51.42999999999999\n",
      "51.43999999999999\n",
      "51.44999999999999\n",
      "51.45999999999999\n",
      "51.469999999999985\n",
      "51.47999999999998\n",
      "51.48999999999998\n",
      "51.49999999999998\n",
      "Added bioclim vars  (20000, 34) \n",
      " {'Vn.tavg_ann', 'Vn.prec_ann'}\n"
     ]
    }
   ],
   "source": [
    "# bioclim\n",
    "df = addWorldClim(df,'ann','Vn.tavg_ann','Vn.prec_ann')\n",
    "print(\"Added bioclim vars \", df.shape, \"\\n\", set(df.columns)-set(ALLVARS))\n",
    "ALLVARS = df.columns\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Export the datagrid as CSV and generate the MD documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>Enb.avg_income</th>\n",
       "      <th>Enb.woz</th>\n",
       "      <th>Enbp.high_education</th>\n",
       "      <th>Enbp.independent</th>\n",
       "      <th>Enbp.low_education</th>\n",
       "      <th>Enbp.unemployed</th>\n",
       "      <th>Gcb.buurt</th>\n",
       "      <th>Gcb.buurtcode</th>\n",
       "      <th>...</th>\n",
       "      <th>Vn.PM10_avg_2020</th>\n",
       "      <th>Vn.PM10_avg_2021</th>\n",
       "      <th>Vn.PM10_avg_2022</th>\n",
       "      <th>Vn.med_tree_height</th>\n",
       "      <th>Vn.number_species</th>\n",
       "      <th>Vn.number_tree_species</th>\n",
       "      <th>Vn.number_trees</th>\n",
       "      <th>Vn.prec_ann</th>\n",
       "      <th>Vn.tavg_ann</th>\n",
       "      <th>Vnp.Green</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51400</td>\n",
       "      <td>5350</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Buiten</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.25</td>\n",
       "      <td>9.924337</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51400</td>\n",
       "      <td>5351</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Buiten</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.25</td>\n",
       "      <td>9.924337</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51400</td>\n",
       "      <td>5352</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Buiten</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.25</td>\n",
       "      <td>9.924337</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51400</td>\n",
       "      <td>5353</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Buiten</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.25</td>\n",
       "      <td>9.924337</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51400</td>\n",
       "      <td>5354</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Buiten</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.25</td>\n",
       "      <td>9.924337</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>51400</td>\n",
       "      <td>5355</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Buiten</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.25</td>\n",
       "      <td>9.924337</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>51400</td>\n",
       "      <td>5356</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Buiten</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.25</td>\n",
       "      <td>9.924337</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>51400</td>\n",
       "      <td>5357</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Buiten</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.25</td>\n",
       "      <td>9.924337</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>51400</td>\n",
       "      <td>5358</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Buiten</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.25</td>\n",
       "      <td>9.924337</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>51400</td>\n",
       "      <td>5359</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Buiten</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.25</td>\n",
       "      <td>9.924337</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     lat   lon Enb.avg_income Enb.woz Enbp.high_education Enbp.independent  \\\n",
       "0  51400  5350            NaN     NaN                 NaN              NaN   \n",
       "1  51400  5351            NaN     NaN                 NaN              NaN   \n",
       "2  51400  5352            NaN     NaN                 NaN              NaN   \n",
       "3  51400  5353            NaN     NaN                 NaN              NaN   \n",
       "4  51400  5354            NaN     NaN                 NaN              NaN   \n",
       "5  51400  5355            NaN     NaN                 NaN              NaN   \n",
       "6  51400  5356            NaN     NaN                 NaN              NaN   \n",
       "7  51400  5357            NaN     NaN                 NaN              NaN   \n",
       "8  51400  5358            NaN     NaN                 NaN              NaN   \n",
       "9  51400  5359            NaN     NaN                 NaN              NaN   \n",
       "\n",
       "  Enbp.low_education Enbp.unemployed Gcb.buurt  Gcb.buurtcode  ...  \\\n",
       "0                NaN             NaN    Buiten              0  ...   \n",
       "1                NaN             NaN    Buiten              0  ...   \n",
       "2                NaN             NaN    Buiten              0  ...   \n",
       "3                NaN             NaN    Buiten              0  ...   \n",
       "4                NaN             NaN    Buiten              0  ...   \n",
       "5                NaN             NaN    Buiten              0  ...   \n",
       "6                NaN             NaN    Buiten              0  ...   \n",
       "7                NaN             NaN    Buiten              0  ...   \n",
       "8                NaN             NaN    Buiten              0  ...   \n",
       "9                NaN             NaN    Buiten              0  ...   \n",
       "\n",
       "   Vn.PM10_avg_2020 Vn.PM10_avg_2021 Vn.PM10_avg_2022 Vn.med_tree_height  \\\n",
       "0               NaN              NaN              NaN                NaN   \n",
       "1               NaN              NaN              NaN                NaN   \n",
       "2               NaN              NaN              NaN                NaN   \n",
       "3               NaN              NaN              NaN                NaN   \n",
       "4               NaN              NaN              NaN                NaN   \n",
       "5               NaN              NaN              NaN                NaN   \n",
       "6               NaN              NaN              NaN                NaN   \n",
       "7               NaN              NaN              NaN                NaN   \n",
       "8               NaN              NaN              NaN                NaN   \n",
       "9               NaN              NaN              NaN                NaN   \n",
       "\n",
       "   Vn.number_species Vn.number_tree_species Vn.number_trees Vn.prec_ann  \\\n",
       "0                NaN                    NaN             NaN       64.25   \n",
       "1                NaN                    NaN             NaN       64.25   \n",
       "2                NaN                    NaN             NaN       64.25   \n",
       "3                NaN                    NaN             NaN       64.25   \n",
       "4                NaN                    NaN             NaN       64.25   \n",
       "5                NaN                    NaN             NaN       64.25   \n",
       "6                NaN                    NaN             NaN       64.25   \n",
       "7                NaN                    NaN             NaN       64.25   \n",
       "8                NaN                    NaN             NaN       64.25   \n",
       "9                NaN                    NaN             NaN       64.25   \n",
       "\n",
       "  Vn.tavg_ann Vnp.Green  \n",
       "0    9.924337       0.0  \n",
       "1    9.924337       0.0  \n",
       "2    9.924337       0.0  \n",
       "3    9.924337       0.0  \n",
       "4    9.924337       0.0  \n",
       "5    9.924337       0.0  \n",
       "6    9.924337       0.0  \n",
       "7    9.924337       0.0  \n",
       "8    9.924337       0.0  \n",
       "9    9.924337       0.0  \n",
       "\n",
       "[10 rows x 34 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort the columns, so that the categories are grouped\n",
    "df_latlon = df.iloc[:, :2]\n",
    "df_etc = df.iloc[:, 2:].sort_index(axis=1)\n",
    "df = pd.concat([df_latlon, df_etc], axis=1)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsaved=df.copy()\n",
    "#df=dfsaved.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Variable | Explanation | Unit | Range | Source |\n",
      "| :--- | :--- | :--- | :--- | :--- | \n",
      "| **lat** | latitude of the bottom-left tile corner x 1000 | latitude degree |51400.0 - 51499.0| |\n",
      "| **lon** | longitude of the bottom-left tile corner x 1000 | longitude degree|5350.0 - 5549.0| |\n",
      "| *Enb.avg_income* | the average income per working individual, in this neighbourhood | euro|9.6 - 150.19| EIC| \n",
      "| *Enb.woz* | average house value | thousand euro|111.0 - 923.0| EIC| \n",
      "| *Enbp.high_education* | percentage of people with high education | %|0.0 - 83.9| EIC| \n",
      "| *Enbp.independent* | percentage of people working | %|50.6 - 87.9| EIC| \n",
      "| *Enbp.low_education* | percentage of people with low education | %|2.6 - 83.3| EIC| \n",
      "| *Enbp.unemployed* | unemployment percentage | %|2.0 - 50.0| EIC| \n",
      "| *Gcb.buurt* | the name of the neighbourhood where this tile belongs to | |'t Hofke - Zwaanstraat| ODE| \n",
      "| *Gcb.buurtcode* | the unique identifier code of the neighbourhood | |0 - 733| ODE| \n",
      "| **Inp.Grey** | the percentage of streets and pavements | %|0.0 - 318.39| ODE- bestrating|\n",
      "| *Pnb.cohesion* | perceived social cohesion score | score from 1 to 10|4.7 - 7.6| EIC| \n",
      "| *Pnb.diversity* | ethnical diversity score | computed index between 0 and 1|0.11 - 0.89| EIC| \n",
      "| *Pnb.good_life* | score for perceived good life | score from 1 to 10|6.5 - 8.5| EIC| \n",
      "| *Pnb.pop* | number of residents/tile in this neigbourhood | residents|0.0 - 94.0| EIC| \n",
      "| *Pnbp.complains_aq* | percentage of people compaining about air quality | %|13.0 - 40.0| EIC| \n",
      "| *Pnbp.complains_noise* | percentage of people compaining about noise | %|24.0 - 76.0| EIC| \n",
      "| *Pnbp.feels_unsafe* | percentage of people that feel unsafe | %|1.0 - 47.0| EIC| \n",
      "| *Pnbp.mostly_by_car* | percentage of people that move mostly by car | %|34.0 - 79.0| EIC| \n",
      "| *Pnbp.mostly_walking* | percentage of people that move mostly by walking | %|3.0 - 46.0| EIC| \n",
      "| *Pnbp.unhappy* | percentage of people that feel unhappy | %|1.0 - 12.0| EIC| \n",
      "| **Vc.milieuzone** | can trucks drive through (True/False) | True/False|0 - 1| ODE|\n",
      "| **Vc.mode_species** | the most observed plant or animal species | |ABIA NITENS - ZINNIA ELEGANS| GBIF|\n",
      "| **Vc.mode_tree_species** | the most common tree species in this tile | |Amberboom - kers| ODE|\n",
      "| **Vn.PM10_avg_2020** | the average PM10 value of all the measurements in 2020 | $\\mu/m^3$|10.98 - 20.84| TNO|\n",
      "| **Vn.PM10_avg_2021** |  | $\\mu/m^3$|14.93 - 21.67| TNO|\n",
      "| **Vn.PM10_avg_2022** |  | $\\mu/m^3$|14.62 - 23.46| TNO|\n",
      "| **Vn.med_tree_height** | the median height of the trees on this tile | m|3.0 - 21.0| ODE- bomen|\n",
      "| **Vn.number_species** | the number of plant, animals, birds and insects species observed on this tile | species|1.0 - 24.0| GBIF|\n",
      "| **Vn.number_tree_species** | the number of different tree species | species|1.0 - 22.0| ODE- bomen|\n",
      "| **Vn.number_trees** | the total number of trees on this tile | trees|1.0 - 111.0| ODE- bomen|\n",
      "| **Vn.prec_ann** | the average rainfall at the centerpoint of this tile in 2022 | mm|63.41 - 64.58| WORLDCLIM|\n",
      "| **Vn.tavg_ann** | the average temperature at the centerpoint of this tile in 2022 | $\\degree C$|9.85 - 10.11| WORLDCLIM|\n",
      "| **Vnp.Green** | the percentage of public green space | %|0.0 - 312.99| ODE- openbaar groen|\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# generate MD documentation\n",
    "printGridInMarkdown(df)\n",
    "#printGridInLatex(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11832, 34)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['Gcb.buurt'] != \"Buiten\"]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save to CSV file\n",
    "\n",
    "df.to_csv(\"ehv-grid-mei23.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "viz-streamlit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
